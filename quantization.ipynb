{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1d39c7",
   "metadata": {},
   "source": [
    "# QAT PROJECT: SST-2 DATASET\n",
    "\n",
    "This file aims to load a model trained with pytorch and convert it to onnx format, the final objective is to apply QAT and observe the trade-offs between the baseline model and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f88ef1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from config import (\n",
    "    FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    TOKENIZED_DATASET_SAVE_PATH, \n",
    "    TOKENIZER_SAVE_PATH, \n",
    "    PER_DEVICE_EVAL_BATCH_SIZE, \n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    SUBSET_SIZE, \n",
    "    NUM_PROCESSES_FOR_MAP, \n",
    "    MAX_SEQUENCE_LENGTH, \n",
    "    MODEL_NAME,\n",
    "    QUANTIZED_QAT_MODEL_SAVE_PATH,\n",
    "    #ONNX_MODEL_SAVE_PATH\n",
    ")\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "SUBSET_SIZE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4df715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ./fine_tuned_baseline_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Model loaded successfully from:\", FINE_TUNED_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8911f47",
   "metadata": {},
   "source": [
    "## TOKENIZATION & DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a39d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Loading tokenizer from local path: ./distilbert_tokenizer_local\n",
      "Loading tokenized dataset from: ./SST2_tokenized_dataset\n",
      "\n",
      "Using full train dataset for training.\n",
      "Final subset sizes: Train=67349, Eval=872\n"
     ]
    }
   ],
   "source": [
    "from src.data_preparation import load_and_preprocess_data, get_subsetted_datasets\n",
    "\n",
    "sst2_ds, tokenized_ds, parent_tokenizer = load_and_preprocess_data(\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_save_path=TOKENIZER_SAVE_PATH,\n",
    "    tokenized_dataset_save_path=TOKENIZED_DATASET_SAVE_PATH,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_processes_for_map=NUM_PROCESSES_FOR_MAP\n",
    ")\n",
    "\n",
    "tok_train_ds, tok_val_ds = get_subsetted_datasets(\n",
    "    tokenized_ds=tokenized_ds,\n",
    "    train_subset_size=SUBSET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928f15d",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d339e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Starting evaluation of the baseline model...\n",
      "Evaluation device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c70f8f7a6d34225b34ab484fb59d65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Accuracy: 0.9000\n",
      "Average Inference Time per Batch: 0.7606 seconds\n",
      "Model Size: 255.43 MB\n",
      "Baseline model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_pytorch_model\n",
    "\n",
    "print(\"\\nStarting evaluation of the baseline model...\")\n",
    "evaluate_pytorch_model(\n",
    "    model_path=FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    tokenizer=parent_tokenizer\n",
    ")\n",
    "print(\"Baseline model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5edb37",
   "metadata": {},
   "source": [
    "## Apply Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05367f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dynamically Quantized Model:\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "layers_to_quantize = {torch.nn.Linear}\n",
    "quantization_dtype = torch.qint8\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model_quantized = torch.quantization.quantize_dynamic(\n",
    "    model=model,\n",
    "    qconfig_spec=layers_to_quantize,\n",
    "    dtype=quantization_dtype\n",
    ")\n",
    "\n",
    "print(\"\\nDynamically Quantized Model:\")\n",
    "print(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f8231",
   "metadata": {},
   "source": [
    "## Saving and loading the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4dbc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model object saved to: ./quantized_pytorch_model\\pytorch_model.bin\n",
      "Quantized model config and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "qat_model_save_path = \"./quantized_pytorch_model\"\n",
    "if not os.path.exists(qat_model_save_path):\n",
    "    os.makedirs(qat_model_save_path)\n",
    "\n",
    "torch.save(model_quantized, os.path.join(qat_model_save_path, \"pytorch_model.bin\"))\n",
    "print(f\"Quantized model object saved to: {os.path.join(qat_model_save_path, 'pytorch_model.bin')}\")\n",
    "\n",
    "# --- Save model config (from original model) and tokenizer (from original model path) ---\n",
    "model.config.save_pretrained(qat_model_save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH) \n",
    "tokenizer.save_pretrained(qat_model_save_path)\n",
    "print(\"Quantized model config and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea33ce",
   "metadata": {},
   "source": [
    "## Export the PyTorch model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa82b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime.configuration import QuantizationConfig, QuantFormat, QuantType\n",
    "\n",
    "model_quantized.eval()\n",
    "\n",
    "quantization_config_onnx = QuantizationConfig(\n",
    "    is_static=True,\n",
    "    per_channel=True,\n",
    "    format=QuantFormat.QDQ,\n",
    "    operators_to_quantize=[\"MatMul\", \"Gemm\"],\n",
    "    weights_symmetric=True,\n",
    "    activations_symmetric=False,\n",
    "    weights_dtype=QuantType.QInt8,\n",
    "    activations_dtype=QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6486b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "# Set the output directory for the final ONNX model\n",
    "output_onnx_dir = \"./onnx_models_quantized\"\n",
    "os.makedirs(output_onnx_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_onnx_dir, \"model.onnx\")\n",
    "\n",
    "main_export(\n",
    "    # Pass the path to the directory containing the FP32 model\n",
    "    model_name_or_path=FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    output=output_onnx_dir,\n",
    "    task=\"sequence-classification\",\n",
    "    tokenizer=parent_tokenizer,\n",
    "    opset=17,\n",
    "    # This tells the exporter to perform quantization during export\n",
    "    quantization_config=quantization_config_onnx,\n",
    "    framework='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933b7d4",
   "metadata": {},
   "source": [
    "## Evaluate the onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46567e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ONNX model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b43e337602a4a4992b2fed30fad47e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ONNX Model:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy on CPU: 0.9083\n",
      "Average Inference Time per Batch on CPU: 1.4485 seconds\n",
      "ONNX Model Size: 255.52 MB\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_onnx_model\n",
    "\n",
    "onnx_metrics, onnx_inference_time, onnx_model_size = evaluate_onnx_model(\n",
    "    onnx_model_path=output_onnx_dir + \"/model.onnx\",\n",
    "    tokenizer=parent_tokenizer,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b41781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
