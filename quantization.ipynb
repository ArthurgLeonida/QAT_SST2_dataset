{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1d39c7",
   "metadata": {},
   "source": [
    "# QAT PROJECT: SST-2 DATASET\n",
    "\n",
    "This file aims to load a model trained with pytorch and convert it to onnx format, the final objective is to apply QAT and observe the trade-offs between the baseline model and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4df715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ./fine_tuned_baseline_model\n"
     ]
    }
   ],
   "source": [
    "from config import FINE_TUNED_MODEL_SAVE_PATH\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Model loaded successfully from:\", FINE_TUNED_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a39d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Loading tokenizer from local path: ./distilbert_tokenizer_local\n",
      "Loading tokenized dataset from: ./SST2_tokenized_dataset\n",
      "\n",
      "Using full train dataset for training.\n",
      "Final subset sizes: Train=67349, Eval=872\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_pytorch_model\n",
    "from src.data_preparation import load_and_preprocess_data, get_subsetted_datasets\n",
    "from config import TOKENIZED_DATASET_SAVE_PATH, TOKENIZER_SAVE_PATH, PER_DEVICE_EVAL_BATCH_SIZE, SUBSET_SIZE, NUM_PROCESSES_FOR_MAP, MAX_SEQUENCE_LENGTH, MODEL_NAME\n",
    "\n",
    "sst2_ds, tokenized_ds, parent_tokenizer = load_and_preprocess_data(\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_save_path=TOKENIZER_SAVE_PATH,\n",
    "    tokenized_dataset_save_path=TOKENIZED_DATASET_SAVE_PATH,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_processes_for_map=NUM_PROCESSES_FOR_MAP\n",
    ")\n",
    "\n",
    "tok_train_ds, tok_val_ds = get_subsetted_datasets(\n",
    "    tokenized_ds=tokenized_ds,\n",
    "    train_subset_size=SUBSET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9fab719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation of the baseline model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4b82d231184e458c07dd9b15095cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Accuracy: 0.9083\n",
      "Average Inference Time per Batch: 0.0360 seconds\n",
      "Model Size: 255.43 MB\n",
      "Baseline model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting evaluation of the baseline model...\")\n",
    "evaluate_pytorch_model(\n",
    "    model_path=FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    tokenizer=parent_tokenizer\n",
    ")\n",
    "print(\"Baseline model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6143b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\onnx\\_internal\\registration.py:159: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "def export_to_onnx(model_name_or_path, output, tokenizer, task=\"sequence-classification\", opset=17):\n",
    "    \"\"\"\n",
    "    Export a model to ONNX format.\n",
    "    \n",
    "    Args:\n",
    "        model_name_or_path (str): Path to the model.\n",
    "        output (str): Output directory for the ONNX model.\n",
    "        task (str): Task type for the model.\n",
    "        tokenizer: Tokenizer used for the model.\n",
    "        opset (int): ONNX opset version.\n",
    "    \"\"\"\n",
    "    main_export(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        output=output,\n",
    "        task=task,\n",
    "        tokenizer=tokenizer,\n",
    "        opset=opset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model prepared for Quantization-Aware Training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.quantization import (\n",
    "    QConfig,\n",
    "    FakeQuantize,\n",
    "    PerChannelMinMaxObserver,\n",
    "    MovingAverageMinMaxObserver\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Exclude embeddings from quantization\n",
    "model.distilbert.embeddings.qconfig = None\n",
    "\n",
    "custom_qconfig = QConfig(\n",
    "    activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8,\n",
    "                                     qscheme=torch.per_tensor_affine),\n",
    "    weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8,\n",
    "                                 qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "\n",
    "model.qconfig = custom_qconfig\n",
    "\n",
    "# qat_model = prepare_qat_fx(model)\n",
    "qat_model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "qat_model.to(device)\n",
    "\n",
    "# Set the model to training mode for the fine-tuning step.\n",
    "qat_model.train()\n",
    "\n",
    "print(\"PyTorch model prepared for Quantization-Aware Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e9a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting QAT fine-tuning for 1 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_95996\\3564176683.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  qat_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8419/8419 26:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.600382</td>\n",
       "      <td>0.899083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from src.utils import compute_metrics\n",
    "\n",
    "num_qat_epochs = 1\n",
    "qat_output_dir = \"./qat_finetuning_output\"\n",
    "qat_learning_rate = 2e-5\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "print(f\"\\nStarting QAT fine-tuning for {num_qat_epochs} epochs...\")\n",
    "\n",
    "qat_training_args = TrainingArguments(\n",
    "    output_dir=qat_output_dir,\n",
    "    num_train_epochs=num_qat_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    learning_rate=qat_learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=f\"{qat_output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "qat_trainer = Trainer(\n",
    "    model=qat_model,\n",
    "    args=qat_training_args,\n",
    "    train_dataset=tok_train_ds,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    tokenizer=parent_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "    \n",
    "qat_trainer.train()\n",
    "print(\"QAT fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533ac92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input model must be a GraphModule, Got type:<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> Please make sure to follow the tutorials.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m qat_model.eval()\n\u001b[32m      4\u001b[39m qat_model.cpu()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m fused_quantized_model = \u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqat_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPyTorch model converted to a fused, quantized state.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:615\u001b[39m, in \u001b[36mconvert_fx\u001b[39m\u001b[34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config, keep_original_weights)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[32m    566\u001b[39m \n\u001b[32m    567\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    612\u001b[39m \n\u001b[32m    613\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    614\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:533\u001b[39m, in \u001b[36m_convert_fx\u001b[39m\u001b[34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[39m\n\u001b[32m    525\u001b[39m     warnings.warn(\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing a convert_custom_config_dict to convert is deprecated and will not be supported \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min a future version. Please pass in a ConvertCustomConfig instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    528\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    529\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    530\u001b[39m     )\n\u001b[32m    531\u001b[39m     convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m \u001b[43m_check_is_graph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m preserved_attr_names = convert_custom_config.preserved_attributes\n\u001b[32m    535\u001b[39m preserved_attrs = {\n\u001b[32m    536\u001b[39m     attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr)\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)\n\u001b[32m    539\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:37\u001b[39m, in \u001b[36m_check_is_graph_module\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_is_graph_module\u001b[39m(model: torch.nn.Module) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, GraphModule):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     38\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minput model must be a GraphModule, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33mGot type:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m             + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model))\n\u001b[32m     41\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33m Please make \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33msure to follow the tutorials.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: input model must be a GraphModule, Got type:<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> Please make sure to follow the tutorials."
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import convert_fx\n",
    "\n",
    "qat_model.eval()\n",
    "qat_model.cpu()\n",
    "fused_quantized_model = convert_fx(qat_model)\n",
    "\n",
    "print(\"\\nPyTorch model converted to a fused, quantized state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb80d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
