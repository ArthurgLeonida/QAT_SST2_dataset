{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1d39c7",
   "metadata": {},
   "source": [
    "# QAT PROJECT: SST-2 DATASET\n",
    "\n",
    "This file aims to load a model trained with pytorch and convert it to onnx format, the final objective is to apply QAT and observe the trade-offs between the baseline model and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88ef1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from config import (\n",
    "    FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    TOKENIZED_DATASET_SAVE_PATH, \n",
    "    TOKENIZER_SAVE_PATH, \n",
    "    PER_DEVICE_EVAL_BATCH_SIZE, \n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    SUBSET_SIZE, \n",
    "    NUM_PROCESSES_FOR_MAP, \n",
    "    MAX_SEQUENCE_LENGTH, \n",
    "    MODEL_NAME,\n",
    "    QUANTIZED_QAT_MODEL_SAVE_PATH,\n",
    "    #ONNX_MODEL_SAVE_PATH\n",
    ")\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "SUBSET_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4df715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ./fine_tuned_baseline_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Model loaded successfully from:\", FINE_TUNED_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a39d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Loading tokenizer from local path: ./distilbert_tokenizer_local\n",
      "Loading tokenized dataset from: ./SST2_tokenized_dataset\n",
      "\n",
      "Using a SUBSET for training (Train size: 100).\n",
      "Final subset sizes: Train=100, Eval=10\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_pytorch_model\n",
    "from src.data_preparation import load_and_preprocess_data, get_subsetted_datasets\n",
    "\n",
    "sst2_ds, tokenized_ds, parent_tokenizer = load_and_preprocess_data(\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_save_path=TOKENIZER_SAVE_PATH,\n",
    "    tokenized_dataset_save_path=TOKENIZED_DATASET_SAVE_PATH,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_processes_for_map=NUM_PROCESSES_FOR_MAP\n",
    ")\n",
    "\n",
    "tok_train_ds, tok_val_ds = get_subsetted_datasets(\n",
    "    tokenized_ds=tokenized_ds,\n",
    "    train_subset_size=SUBSET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9fab719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation of the baseline model...\n",
      "Evaluation device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270e783b253145ecaff8f452608ecdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Accuracy: 0.9000\n",
      "Average Inference Time per Batch: 0.5241 seconds\n",
      "Model Size: 255.43 MB\n",
      "Baseline model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting evaluation of the baseline model...\")\n",
    "evaluate_pytorch_model(\n",
    "    model_path=FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    tokenizer=parent_tokenizer\n",
    ")\n",
    "print(\"Baseline model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6143b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "def export_to_onnx(model_name_or_path, output, tokenizer, task=\"sequence-classification\", opset=17):\n",
    "    \"\"\"\n",
    "    Export a model to ONNX format.\n",
    "    \n",
    "    Args:\n",
    "        model_name_or_path (str): Path to the model.\n",
    "        output (str): Output directory for the ONNX model.\n",
    "        task (str): Task type for the model.\n",
    "        tokenizer: Tokenizer used for the model.\n",
    "        opset (int): ONNX opset version.\n",
    "    \"\"\"\n",
    "    main_export(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        output=output,\n",
    "        task=task,\n",
    "        tokenizer=tokenizer,\n",
    "        opset=opset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee4f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model prepared for Quantization-Aware Training.\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization import (\n",
    "    QConfig,\n",
    "    FakeQuantize,\n",
    "    PerChannelMinMaxObserver,\n",
    "    MovingAverageMinMaxObserver\n",
    ")\n",
    "from torch.ao.quantization.qconfig_mapping import QConfigMapping\n",
    "from torch.ao.quantization import get_default_qconfig, prepare_qat, convert\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Exclude embeddings from quantization\n",
    "model.distilbert.embeddings.qconfig = None\n",
    "\n",
    "\n",
    "custom_qconfig = QConfig(\n",
    "    activation=FakeQuantize.with_args(\n",
    "        observer=MovingAverageMinMaxObserver, \n",
    "        quant_min=0, \n",
    "        quant_max=255, \n",
    "        dtype=torch.quint8\n",
    "    ),\n",
    "    weight=FakeQuantize.with_args(\n",
    "        observer=MovingAverageMinMaxObserver, \n",
    "        quant_min=-128, \n",
    "        quant_max=127, \n",
    "        dtype=torch.qint8,\n",
    "        qscheme=torch.per_tensor_symmetric)\n",
    ")\n",
    "\n",
    "model.qconfig = custom_qconfig\n",
    "\n",
    "# qat_model = prepare_qat_fx(model)\n",
    "qat_model = prepare_qat(model, inplace=False)\n",
    "qat_model.to(device)\n",
    "\n",
    "print(\"PyTorch model prepared for Quantization-Aware Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e9a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting QAT fine-tuning for 2 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 01:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.081199</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from src.utils import compute_metrics\n",
    "\n",
    "num_qat_epochs = 2\n",
    "qat_output_dir = \"./qat_finetuning_output\"\n",
    "qat_learning_rate = 2e-5\n",
    "\n",
    "print(f\"\\nStarting QAT fine-tuning for {num_qat_epochs} epochs...\")\n",
    "\n",
    "qat_training_args = TrainingArguments(\n",
    "    output_dir=qat_output_dir,\n",
    "    num_train_epochs=num_qat_epochs,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    learning_rate=qat_learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=f\"{qat_output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "qat_trainer = Trainer(\n",
    "    model=qat_model,\n",
    "    args=qat_training_args,\n",
    "    train_dataset=tok_train_ds,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    processing_class=parent_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "qat_trainer.train()\n",
    "print(\"QAT fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be1ef80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting QAT-trained model to final quantized version...\n",
      "Model converted to final quantized version.\n",
      "QAT-trained model object saved to: ./quantized_pytorch_model\\pytorch_model.bin\n",
      "QAT model config and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "# --- Convert the model to its final quantized version ---\n",
    "print(\"\\nConverting QAT-trained model to final quantized version...\")\n",
    "qat_model.eval() \n",
    "quantized_model_obj = torch.quantization.convert(model, inplace=True)\n",
    "print(\"Model converted to final quantized version.\")\n",
    "\n",
    "qat_model_save_path = \"./quantized_pytorch_model\"\n",
    "\n",
    "# --- Save the QAT-trained and converted model ---\n",
    "if not os.path.exists(qat_model_save_path):\n",
    "    os.makedirs(qat_model_save_path)\n",
    "\n",
    "torch.save(quantized_model_obj.state_dict(), os.path.join(qat_model_save_path, \"pytorch_model.bin\"))\n",
    "print(f\"QAT-trained model object saved to: {os.path.join(qat_model_save_path, 'pytorch_model.bin')}\")\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "original_config.save_pretrained(qat_model_save_path)\n",
    "tokenizer_obj = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "tokenizer_obj.save_pretrained(qat_model_save_path)\n",
    "print(\"QAT model config and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "533ac92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime.configuration import QuantizationConfig, QuantFormat, QuantType\n",
    "\n",
    "qat_model.eval()\n",
    "\n",
    "quantization_config_onnx = QuantizationConfig(\n",
    "    is_static=True,\n",
    "    per_channel=True,\n",
    "    format=QuantFormat.QDQ,\n",
    "    operators_to_quantize=[\"MatMul\", \"Gemm\"],\n",
    "    weights_symmetric=True,\n",
    "    activations_symmetric=False,\n",
    "    weights_dtype=QuantType.QInt8,\n",
    "    activations_dtype=QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67bb80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "# Set the output directory for the final ONNX model\n",
    "output_onnx_dir = \"./onnx_models_quantized\"\n",
    "os.makedirs(output_onnx_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_onnx_dir, \"model.onnx\")\n",
    "\n",
    "main_export(\n",
    "    # Pass the path to the directory containing the FP32 model\n",
    "    model_name_or_path=qat_model_save_path,\n",
    "    output=output_onnx_dir,\n",
    "    task=\"sequence-classification\",\n",
    "    tokenizer=parent_tokenizer,\n",
    "    opset=17,\n",
    "    # This tells the exporter to perform quantization during export\n",
    "    quantization_config=quantization_config_onnx,\n",
    "    library_name='transformers',\n",
    "    framework='pt',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46567e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ONNX model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61bdd2232bb4c86b1ec55c78a146a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ONNX Model:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy on CPU: 0.9000\n",
      "Average Inference Time per Batch on CPU: 0.8408 seconds\n",
      "ONNX Model Size: 255.52 MB\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_onnx_model\n",
    "\n",
    "onnx_metrics, onnx_inference_time, onnx_model_size = evaluate_onnx_model(\n",
    "    onnx_model_path=output_onnx_dir + \"/model.onnx\",\n",
    "    tokenizer=parent_tokenizer,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea11396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
