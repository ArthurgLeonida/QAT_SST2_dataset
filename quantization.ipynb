{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1d39c7",
   "metadata": {},
   "source": [
    "# QAT PROJECT: SST-2 DATASET\n",
    "\n",
    "This file aims to load a model trained with pytorch and convert it to onnx format, the final objective is to apply QAT and observe the trade-offs between the baseline model and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88ef1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    TOKENIZED_DATASET_SAVE_PATH, \n",
    "    TOKENIZER_SAVE_PATH, \n",
    "    PER_DEVICE_EVAL_BATCH_SIZE, \n",
    "    SUBSET_SIZE, \n",
    "    NUM_PROCESSES_FOR_MAP, \n",
    "    MAX_SEQUENCE_LENGTH, \n",
    "    MODEL_NAME,\n",
    "    QUANTIZED_QAT_MODEL_SAVE_PATH\n",
    ")\n",
    "\n",
    "SUBSET_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4df715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ./fine_tuned_baseline_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Model loaded successfully from:\", FINE_TUNED_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a39d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Loading tokenizer from local path: ./distilbert_tokenizer_local\n",
      "Loading tokenized dataset from: ./SST2_tokenized_dataset\n",
      "\n",
      "Using a SUBSET for training (Train size: 100).\n",
      "Final subset sizes: Train=100, Eval=10\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_pytorch_model\n",
    "from src.data_preparation import load_and_preprocess_data, get_subsetted_datasets\n",
    "\n",
    "sst2_ds, tokenized_ds, parent_tokenizer = load_and_preprocess_data(\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_save_path=TOKENIZER_SAVE_PATH,\n",
    "    tokenized_dataset_save_path=TOKENIZED_DATASET_SAVE_PATH,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_processes_for_map=NUM_PROCESSES_FOR_MAP\n",
    ")\n",
    "\n",
    "tok_train_ds, tok_val_ds = get_subsetted_datasets(\n",
    "    tokenized_ds=tokenized_ds,\n",
    "    train_subset_size=SUBSET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9fab719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation of the baseline model...\n",
      "Evaluation device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f30484836f41d799547bfb98fd62ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Accuracy: 0.9000\n",
      "Average Inference Time per Batch: 0.0243 seconds\n",
      "Model Size: 255.43 MB\n",
      "Baseline model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting evaluation of the baseline model...\")\n",
    "evaluate_pytorch_model(\n",
    "    model_path=FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    tokenizer=parent_tokenizer\n",
    ")\n",
    "print(\"Baseline model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6143b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "def export_to_onnx(model_name_or_path, output, tokenizer, task=\"sequence-classification\", opset=17):\n",
    "    \"\"\"\n",
    "    Export a model to ONNX format.\n",
    "    \n",
    "    Args:\n",
    "        model_name_or_path (str): Path to the model.\n",
    "        output (str): Output directory for the ONNX model.\n",
    "        task (str): Task type for the model.\n",
    "        tokenizer: Tokenizer used for the model.\n",
    "        opset (int): ONNX opset version.\n",
    "    \"\"\"\n",
    "    main_export(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        output=output,\n",
    "        task=task,\n",
    "        tokenizer=tokenizer,\n",
    "        opset=opset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ee4f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model prepared for Quantization-Aware Training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.quantization import (\n",
    "    QConfig,\n",
    "    FakeQuantize,\n",
    "    PerChannelMinMaxObserver,\n",
    "    MovingAverageMinMaxObserver\n",
    ")\n",
    "from torch.ao.quantization.qconfig_mapping import QConfigMapping\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Exclude embeddings from quantization\n",
    "model.distilbert.embeddings.qconfig = None\n",
    "\n",
    "custom_qconfig = QConfig(\n",
    "    activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8,\n",
    "                                     qscheme=torch.per_tensor_affine),\n",
    "    weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8,\n",
    "                                 qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "\n",
    "model.qconfig = custom_qconfig\n",
    "\n",
    "# qat_model = prepare_qat_fx(model)\n",
    "qat_model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "qat_model.to(device)\n",
    "\n",
    "# Set the model to training mode for the fine-tuning step.\n",
    "qat_model.train()\n",
    "\n",
    "print(\"PyTorch model prepared for Quantization-Aware Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting QAT fine-tuning for 2 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_2392\\1319999894.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  qat_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT fine-tuning complete.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m qat_trainer.train()\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQAT fine-tuning complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m quantized_model_obj = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqat_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m torch.save(quantized_model_obj, QUANTIZED_QAT_MODEL_SAVE_PATH)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuantized model saved in pytorch format at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUANTIZED_QAT_MODEL_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:655\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m    654\u001b[39m     module = copy.deepcopy(module)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[32m    664\u001b[39m     _remove_qconfig(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:712\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m     reassign[name] = swap_module(\n\u001b[32m    721\u001b[39m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[32m    722\u001b[39m     )\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:712\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m     reassign[name] = swap_module(\n\u001b[32m    721\u001b[39m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[32m    722\u001b[39m     )\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
      "    \u001b[31m[... skipping similar frames: _convert at line 712 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:712\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m     reassign[name] = swap_module(\n\u001b[32m    721\u001b[39m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[32m    722\u001b[39m     )\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:720\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m    712\u001b[39m         _convert(\n\u001b[32m    713\u001b[39m             mod,\n\u001b[32m    714\u001b[39m             mapping,\n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m             use_precomputed_fake_quant=use_precomputed_fake_quant,\n\u001b[32m    719\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     reassign[name] = \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n\u001b[32m    725\u001b[39m     module._modules[key] = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:762\u001b[39m, in \u001b[36mswap_module\u001b[39m\u001b[34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    760\u001b[39m sig = inspect.signature(qmod.from_float)\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_precomputed_fake_quant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig.parameters:\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m     new_mod = \u001b[43mqmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    766\u001b[39m     new_mod = qmod.from_float(mod)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\linear.py:341\u001b[39m, in \u001b[36mLinear.from_float\u001b[39m\u001b[34m(cls, mod, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    339\u001b[39m qweight = _quantize_weight(mod.weight.float(), weight_post_process)\n\u001b[32m    340\u001b[39m qlinear = \u001b[38;5;28mcls\u001b[39m(mod.in_features, mod.out_features, dtype=dtype)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[43mqlinear\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m qlinear.scale = \u001b[38;5;28mfloat\u001b[39m(act_scale)\n\u001b[32m    343\u001b[39m qlinear.zero_point = \u001b[38;5;28mint\u001b[39m(act_zp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\linear.py:282\u001b[39m, in \u001b[36mLinear.set_weight_bias\u001b[39m\u001b[34m(self, w, b)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch.Tensor, b: Optional[torch.Tensor]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\linear.py:38\u001b[39m, in \u001b[36mLinearPackedParams.set_weight_bias\u001b[39m\u001b[34m(self, weight, bias)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;129m@torch\u001b[39m.jit.export\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_weight_bias\u001b[39m(\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m, weight: torch.Tensor, bias: Optional[torch.Tensor]\n\u001b[32m     36\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.qint8:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28mself\u001b[39m._packed_params = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.float16:\n\u001b[32m     40\u001b[39m         \u001b[38;5;28mself\u001b[39m._packed_params = torch.ops.quantized.linear_prepack_fp16(weight, bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from src.utils import compute_metrics\n",
    "\n",
    "num_qat_epochs = 2\n",
    "qat_output_dir = \"./qat_finetuning_output\"\n",
    "qat_learning_rate = 2e-5\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "\n",
    "print(f\"\\nStarting QAT fine-tuning for {num_qat_epochs} epochs...\")\n",
    "\n",
    "qat_training_args = TrainingArguments(\n",
    "    output_dir=qat_output_dir,\n",
    "    num_train_epochs=num_qat_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    learning_rate=qat_learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=f\"{qat_output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "qat_trainer = Trainer(\n",
    "    model=qat_model,\n",
    "    args=qat_training_args,\n",
    "    train_dataset=tok_train_ds,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    processing_class=parent_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "    \n",
    "qat_trainer.train()\n",
    "print(\"QAT fine-tuning complete.\")\n",
    "\n",
    "quantized_model_obj = torch.quantization.convert(qat_model, inplace=True)\n",
    "torch.save(quantized_model_obj, QUANTIZED_QAT_MODEL_SAVE_PATH)\n",
    "print(f\"Quantized model saved in pytorch format at: {QUANTIZED_QAT_MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533ac92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input model must be a GraphModule, Got type:<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> Please make sure to follow the tutorials.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m qat_model.eval()\n\u001b[32m      4\u001b[39m qat_model.cpu()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m fused_quantized_model = \u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqat_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPyTorch model converted to a fused, quantized state.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:615\u001b[39m, in \u001b[36mconvert_fx\u001b[39m\u001b[34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config, keep_original_weights)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[32m    566\u001b[39m \n\u001b[32m    567\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    612\u001b[39m \n\u001b[32m    613\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    614\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:533\u001b[39m, in \u001b[36m_convert_fx\u001b[39m\u001b[34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[39m\n\u001b[32m    525\u001b[39m     warnings.warn(\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing a convert_custom_config_dict to convert is deprecated and will not be supported \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min a future version. Please pass in a ConvertCustomConfig instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    528\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    529\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    530\u001b[39m     )\n\u001b[32m    531\u001b[39m     convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m \u001b[43m_check_is_graph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m preserved_attr_names = convert_custom_config.preserved_attributes\n\u001b[32m    535\u001b[39m preserved_attrs = {\n\u001b[32m    536\u001b[39m     attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr)\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)\n\u001b[32m    539\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\Documents\\Estudos\\Model_Compression\\QAT_SST2_dataset\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:37\u001b[39m, in \u001b[36m_check_is_graph_module\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_is_graph_module\u001b[39m(model: torch.nn.Module) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, GraphModule):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     38\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minput model must be a GraphModule, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33mGot type:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m             + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model))\n\u001b[32m     41\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33m Please make \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33msure to follow the tutorials.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: input model must be a GraphModule, Got type:<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> Please make sure to follow the tutorials."
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import convert_fx\n",
    "\n",
    "qat_model.eval()\n",
    "qat_model.cpu()\n",
    "fused_quantized_model = convert_fx(qat_model)\n",
    "\n",
    "print(\"\\nPyTorch model converted to a fused, quantized state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb80d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
