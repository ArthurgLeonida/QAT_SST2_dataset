{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1d39c7",
   "metadata": {},
   "source": [
    "# QAT PROJECT: SST-2 DATASET\n",
    "\n",
    "This file aims to load a model trained with pytorch and convert it to onnx format, the final objective is to apply QAT and observe the trade-offs between the baseline model and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4df715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ./fine_tuned_baseline_model\n"
     ]
    }
   ],
   "source": [
    "from config import FINE_TUNED_MODEL_SAVE_PATH\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Model loaded successfully from:\", FINE_TUNED_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a39d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Loading tokenizer from local path: ./distilbert_tokenizer_local\n",
      "Loading tokenized dataset from: ./SST2_tokenized_dataset\n",
      "\n",
      "Using full train dataset for training.\n",
      "Final subset sizes: Train=67349, Eval=872\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate_models import evaluate_pytorch_model\n",
    "from src.data_preparation import load_and_preprocess_data, get_subsetted_datasets\n",
    "from config import TOKENIZED_DATASET_SAVE_PATH, TOKENIZER_SAVE_PATH, PER_DEVICE_EVAL_BATCH_SIZE, SUBSET_SIZE, NUM_PROCESSES_FOR_MAP, MAX_SEQUENCE_LENGTH, MODEL_NAME\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sst2_ds, tokenized_ds, parent_tokenizer = load_and_preprocess_data(\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_save_path=TOKENIZER_SAVE_PATH,\n",
    "    tokenized_dataset_save_path=TOKENIZED_DATASET_SAVE_PATH,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_processes_for_map=NUM_PROCESSES_FOR_MAP\n",
    ")\n",
    "\n",
    "tok_train_ds, tok_val_ds = get_subsetted_datasets(\n",
    "    tokenized_ds=tokenized_ds,\n",
    "    train_subset_size=SUBSET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9fab719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation of the baseline model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de0eb90d6be4158a533a132fdc28105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Accuracy: 0.9083\n",
      "Average Inference Time per Batch: 1.8297 seconds\n",
      "Model Size: 255.43 MB\n",
      "Baseline model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting evaluation of the baseline model...\")\n",
    "evaluate_pytorch_model(\n",
    "    model_path=FINE_TUNED_MODEL_SAVE_PATH,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    tokenizer=parent_tokenizer\n",
    ")\n",
    "print(\"Baseline model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6143b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "import os\n",
    "\n",
    "def export_to_onnx(model_name_or_path, output, tokenizer, task=\"sequence-classification\", opset=17):\n",
    "    \"\"\"\n",
    "    Export a model to ONNX format.\n",
    "    \n",
    "    Args:\n",
    "        model_name_or_path (str): Path to the model.\n",
    "        output (str): Output directory for the ONNX model.\n",
    "        task (str): Task type for the model.\n",
    "        tokenizer: Tokenizer used for the model.\n",
    "        opset (int): ONNX opset version.\n",
    "    \"\"\"\n",
    "    main_export(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        output=output,\n",
    "        task=task,\n",
    "        tokenizer=tokenizer,\n",
    "        opset=opset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ee4f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model prepared for Quantization-Aware Training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.quantization import (\n",
    "    QConfig,\n",
    "    FakeQuantize,\n",
    "    PerChannelMinMaxObserver,\n",
    "    MovingAverageMinMaxObserver\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Exclude embeddings from quantization\n",
    "model.distilbert.embeddings.qconfig = None\n",
    "\n",
    "custom_qconfig = QConfig(\n",
    "    activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8,\n",
    "                                     qscheme=torch.per_tensor_affine),\n",
    "    weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8,\n",
    "                                 qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "\n",
    "model.qconfig = custom_qconfig\n",
    "\n",
    "qat_model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "qat_model.to(device)\n",
    "\n",
    "# Set the model to training mode for the fine-tuning step.\n",
    "qat_model.train()\n",
    "\n",
    "print(\"PyTorch model prepared for Quantization-Aware Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00e9a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting QAT fine-tuning for 1 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_4392\\3474524333.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  qat_trainer = Trainer(\n",
      "e:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/8419 00:14 < 5:36:58, 0.42 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     13\u001b[39m qat_training_args = TrainingArguments(\n\u001b[32m     14\u001b[39m     output_dir=qat_output_dir,\n\u001b[32m     15\u001b[39m     num_train_epochs=num_qat_epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mtensorboard\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m qat_trainer = Trainer(\n\u001b[32m     32\u001b[39m     model=qat_model,\n\u001b[32m     33\u001b[39m     args=qat_training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mqat_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQAT fine-tuning complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m \u001b[38;5;28mself\u001b[39m._train_batch_size = batch_size\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.auto_find_batch_size:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.train_batch_size != \u001b[38;5;28mself\u001b[39m._train_batch_size:\n\u001b[32m   2241\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m release_memory\n\u001b[32m   2243\u001b[39m         (\u001b[38;5;28mself\u001b[39m.model_wrapped,) = release_memory(\u001b[38;5;28mself\u001b[39m.model_wrapped)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[39m, in \u001b[36m_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2547\u001b[39m with context():\n\u001b[32m   2548\u001b[39m     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2550\u001b[39m if (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     and not is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     # if loss is nan or inf simply add the average of previous logged losses\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n\u001b[32m   2557\u001b[39m else:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\trainer.py:3745\u001b[39m, in \u001b[36mtraining_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3743\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._prepare_inputs(inputs)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss_mb = smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps)\n\u001b[32m   3746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\trainer.py:3810\u001b[39m, in \u001b[36mcompute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\n\u001b[32m   3802\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3803\u001b[39m     model: nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3806\u001b[39m     num_items_in_batch: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3807\u001b[39m ):\n\u001b[32m   3808\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3809\u001b[39m \u001b[33;03m    How the loss is computed by Trainer. By default, all models return the loss in the first element.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m \n\u001b[32m   3811\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[33;03m        model (`nn.Module`):\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[33;03m            The model to compute the loss for.\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m        inputs (`dict[str, Union[torch.Tensor, Any]]`):\u001b[39;00m\n\u001b[32m   3815\u001b[39m \u001b[33;03m            The input data for the model.\u001b[39;00m\n\u001b[32m   3816\u001b[39m \u001b[33;03m        return_outputs (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[32m   3817\u001b[39m \u001b[33;03m            Whether to return the model outputs along with the loss.\u001b[39;00m\n\u001b[32m   3818\u001b[39m \u001b[33;03m        num_items_in_batch (Optional[torch.Tensor], *optional*):\u001b[39;00m\n\u001b[32m   3819\u001b[39m \u001b[33;03m            The number of items in the batch. If num_items_in_batch is not passed,\u001b[39;00m\n\u001b[32m   3820\u001b[39m \n\u001b[32m   3821\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   3822\u001b[39m \u001b[33;03m        The loss of the model along with its output if return_outputs was set to True\u001b[39;00m\n\u001b[32m   3823\u001b[39m \n\u001b[32m   3824\u001b[39m \u001b[33;03m    Subclass and override for custom behavior. If you are not using `num_items_in_batch` when computing your loss,\u001b[39;00m\n\u001b[32m   3825\u001b[39m \u001b[33;03m    make sure to overwrite `self.model_accepts_loss_kwargs` to `False`. Otherwise, the loss calculationg might be slightly inacurate when performing gradient accumulation.\u001b[39;00m\n\u001b[32m   3826\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.label_smoother \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[32m   3828\u001b[39m         labels = inputs.pop(\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:920\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    910\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    913\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    915\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    917\u001b[39m distilbert_output = \u001b[38;5;28mself\u001b[39m.distilbert(\n\u001b[32m    918\u001b[39m     input_ids=input_ids,\n\u001b[32m    919\u001b[39m     attention_mask=attention_mask,\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m     head_mask=head_mask,\n\u001b[32m    921\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    922\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    923\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    924\u001b[39m     return_dict=return_dict,\n\u001b[32m    925\u001b[39m )\n\u001b[32m    926\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    927\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:739\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    732\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    733\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    734\u001b[39m         )\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m    737\u001b[39m     x=embeddings,\n\u001b[32m    738\u001b[39m     attn_mask=attention_mask,\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m     head_mask=head_mask,\n\u001b[32m    740\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    741\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    742\u001b[39m     return_dict=return_dict,\n\u001b[32m    743\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:544\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    539\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m    541\u001b[39m layer_outputs = layer_module(\n\u001b[32m    542\u001b[39m     hidden_state,\n\u001b[32m    543\u001b[39m     attn_mask,\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     head_mask[i],\n\u001b[32m    545\u001b[39m     output_attentions,\n\u001b[32m    546\u001b[39m )\n\u001b[32m    548\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:488\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# To handle these `output_attentions` or `output_hidden_states` cases returning tuples\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(sa_output) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msa_output must be a tuple but it is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sa_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    490\u001b[39m     sa_output = sa_output[\u001b[32m0\u001b[39m]\n\u001b[32m    491\u001b[39m sa_output = \u001b[38;5;28mself\u001b[39m.sa_layer_norm(sa_output + x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:422\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28mself\u001b[39m.dropout = nn.Dropout(p=config.dropout)\n\u001b[32m    421\u001b[39m \u001b[38;5;28mself\u001b[39m.chunk_size_feed_forward = config.chunk_size_feed_forward\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m \u001b[38;5;28mself\u001b[39m.seq_len_dim = \u001b[32m1\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n\u001b[32m    424\u001b[39m \u001b[38;5;28mself\u001b[39m.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(*input_tensors)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_pruneable_heads_and_indices\u001b[39m(\n\u001b[32m    254\u001b[39m     heads: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], n_heads: \u001b[38;5;28mint\u001b[39m, head_size: \u001b[38;5;28mint\u001b[39m, already_pruned_heads: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mint\u001b[39m]\n\u001b[32m    255\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mint\u001b[39m], torch.LongTensor]:\n\u001b[32m    256\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[33;03m    Finds the heads and their indices taking `already_pruned_heads` into account.\u001b[39;00m\n\u001b[32m    258\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m \u001b[33;03m        into account and the indices of rows/columns to keep in the layer weight.\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m     mask = torch.ones(n_heads, head_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:425\u001b[39m, in \u001b[36mff_chunk\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n\u001b[32m    424\u001b[39m \u001b[38;5;28mself\u001b[39m.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[38;5;28mself\u001b[39m.activation = get_activation(config.activation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\ao\\nn\\qat\\modules\\linear.py:49\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_fake_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos_HD\\Estudo\\ModelCompression\\QAT_ONNX\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\fake_quantize.py:408\u001b[39m, in \u001b[36mFusedMovingAvgObsFakeQuantize.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfused_moving_avg_obs_fake_quant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobserver_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfake_quant_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43maveraging_constant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquant_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquant_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mch_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_per_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_symmetric_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from src.utils import compute_metrics\n",
    "\n",
    "num_qat_epochs = 1\n",
    "qat_output_dir = \"./qat_finetuning_output\"\n",
    "qat_learning_rate = 2e-5\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "max_train_steps = -1  # -1 means no limit\n",
    "\n",
    "print(f\"\\nStarting QAT fine-tuning for {num_qat_epochs} epochs...\")\n",
    "\n",
    "qat_training_args = TrainingArguments(\n",
    "    output_dir=qat_output_dir,\n",
    "    num_train_epochs=num_qat_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    learning_rate=qat_learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=f\"{qat_output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "qat_trainer = Trainer(\n",
    "    model=qat_model,\n",
    "    args=qat_training_args,\n",
    "    train_dataset=tok_train_ds,\n",
    "    eval_dataset=tok_val_ds,\n",
    "    tokenizer=parent_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "    \n",
    "qat_trainer.train()\n",
    "print(\"QAT fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ac92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
